{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first preliminary step to performing any kind of sentiment analysis on Reddit data is establishing a Reddit instance via API. This can be done with the Python Reddit API Wrapper (PRAW), see documentation: https://praw.readthedocs.io/en/latest/\n",
    "\n",
    "In order to access the API, a Reddit account is needed, login details below.\n",
    "E-mail: barbara.balcon@studbocconi.it\n",
    "Username: thesis_3078976\n",
    "Password: class_of_2021\n",
    "\n",
    "I created an application at the following link: https://www.reddit.com/prefs/apps\n",
    "Name: Sentiment Analysis\n",
    "I selected 'script' (Script for personal use. Will only have access to the developers accounts\n",
    "description)\n",
    "Description: Data will be used to perform sentiment analysis\n",
    "About url: blank\n",
    "Redirect url: http://www.example.com/unused/redirect/url (this is empty)\n",
    "\n",
    "After creating the app, the following is generated.\n",
    "ID: L_wWPUNiMBmn1Q\n",
    "Secret: bUYKyV21W3xXK4GzXsY5tsNzpG3pRw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import of the relevant libraries \n",
    "import praw\n",
    "import pandas as pd\n",
    "from praw.models import MoreComments\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "import emoji #remove emoji\n",
    "import re #remove links\n",
    "#import en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a reddit connection with reddit api details\n",
    "reddit=praw.Reddit(client_id='L_wWPUNiMBmn1Q', client_secret='bUYKyV21W3xXK4GzXsY5tsNzpG3pRw', user_agent='ua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Are Your Moves Tomorrow, March 29, 2021\n",
      "Submission ID =  mf8mct \n",
      "\n",
      "Join /r/WallStreetBets' Third Annual Paper Trading Competition!\n",
      "Submission ID =  mdz6oc \n",
      "\n",
      "GameStop gear going to the new home at the Childrenâ€™s hospital of Atlanta Monday. POWER TO THE PLAYERS !\n",
      "Submission ID =  mf0msr \n",
      "\n",
      "Watch out for April\n",
      "Submission ID =  mf4qpj \n",
      "\n",
      "Generoustreetbets\n",
      "Submission ID =  mf6rfl \n",
      "\n"
     ]
    }
   ],
   "source": [
    "subreddit=reddit.subreddit('wallstreetbets')\n",
    "for submission in subreddit.hot(limit=5):\n",
    "    print(submission.title)\n",
    "    print('Submission ID = ', submission.id, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
